## cluster
>聚类(Clustering)是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。

### 聚类和分类：
* 聚类(Clustering)：是指把相似的数据划分到一起，具体划分的时候并不关心这一类的标签，目标就是把相似的数据聚合到一起，聚类是一种无监督学习(Unsupervised Learning)方法。
* 分类(Classification)：是把不同的数据划分开，其过程是通过训练数据集获得一个分类器，再通过分类器去预测未知数据，分类是一种监督学习(Supervised Learning)方法。

### 划分式
k-means算法：  
1. 创建k个点作为初始质心
2. 当任意一个点的簇划分结果发生改变时  
    1. 对数据集中的每个数据点  
        1. 对每个质心  
            1. 计算质心与数据点之间的距离
        2. 将数据点分配到距其最近的簇
    2. 对每个簇，计算所有点的均值并将其作为质心

#### 凸函数与非凸函数:
* 鞍点是参数空间中损失函数在一个方向上具有最小值而在另一个方向上具有最大值的点。在鞍点处，损失函数的梯度为零，这意味着优化算法可能会卡住，无法收敛到全局最小值。  
* 任何优化算法的目标都是找到全局最小值，这将为给定问题产生最佳结果。
* 非凸函数的挑战在于它们可以有多个局部最小值，这些点是函数值低于所有相邻点的点。
这意味着如果我们尝试优化一个非凸函数，我们可能会陷入局部最小值而错过全局最小值，而这正是我们正在寻找的最优解。

k-mean++:
* k-means++是针对k-means中初始质心点选取的优化算法。该算法的流程和k-means类似，改变的地方只有初始质心的选取，该部分的算法流程如下：  
1. 随机选取一个数据点作为初始的聚类中心
2. 当聚类中心数量小于k
    1. 计算每个数据点与当前已有的聚类中心的最短距离，用D(x)表示，这个值越大，表示被选为聚类中心的可能性越大，最后使用轮盘法选取下一个聚类中心。  

### EM算法：
* EM算法的核心思想非常简单，分为两步：Expection-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后似然函数都会增加，所以函数最终会收敛。

